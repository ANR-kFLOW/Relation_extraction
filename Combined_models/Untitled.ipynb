{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "ce990cc5-2633-461d-9f55-58890e40e392",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "13dd3bfa-d4e0-4733-9bc3-6f21c6882c6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     index  prediction\n",
      "0        0           0\n",
      "1        1           0\n",
      "2        2           0\n",
      "3        3           0\n",
      "4        4           0\n",
      "..     ...         ...\n",
      "455    455           0\n",
      "456    456           0\n",
      "457    457           0\n",
      "458    458           0\n",
      "459    459           0\n",
      "\n",
      "[460 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('outs/2sft_st1_base_new/predict_results_cola.txt', delimiter='\\t')\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d3d043b-7a7d-497f-889e-6713cef1686d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index  prediction\n",
       "0      0           0\n",
       "1      1           0\n",
       "2      2           0\n",
       "3      3           0\n",
       "4      4           0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6db8eaa-7312-459d-9859-aca44e93fc2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['test'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "13ac1306-885d-43a7-94ba-9858e648fbdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>prediction</th>\n",
       "      <th>test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index  prediction  test\n",
       "0      0           0     0\n",
       "1      1           0     0\n",
       "2      2           0     0\n",
       "3      3           0     0\n",
       "4      4           0     0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6509581a-82a0-45e1-8b7c-58188a255129",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>prediction</th>\n",
       "      <th>test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index  prediction  test\n",
       "0      0           0     0\n",
       "1      1           0     1\n",
       "2      2           0     2\n",
       "3      3           0     3\n",
       "4      4           0     0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[1,'test'] = 1\n",
    "df.loc[2,'test'] = 2\n",
    "df.loc[3,'test'] = 3\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "0551c785-fb51-433a-88e3-0d67265bd13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('only_causal.csv')\n",
    "df_dup = df\n",
    "df_dup['dup'] = 0\n",
    "i = 0\n",
    "i_d = 0\n",
    "while i < len(df):\n",
    "    c = df.loc[i,'num_rs']\n",
    "        \n",
    "    if c > 1:\n",
    "        new_row_df = pd.DataFrame(df.loc[[i],:])\n",
    "            #print('here')\n",
    "            #print(type(new_row_df))\n",
    "        for q in range(c):\n",
    "            df_before =  pd.DataFrame(df_dup.iloc[:i_d+1, :])\n",
    "            df_after =  pd.DataFrame(df_dup.iloc[i_d+1:, :])\n",
    "                #print(len(df_before))\n",
    "                #print(len(df_after))\n",
    "                #print(type(df_before))\n",
    "                #print(type(df_after))\n",
    "            new_row_i = new_row_df\n",
    "            new_row_i['dup'] = q + 1\n",
    "            df_dup = pd.concat([df_before, new_row_i, df_after]).reset_index(drop=True)\n",
    "            i_d = i_d + 1\n",
    "    i = 100000000\n",
    "                \n",
    "    i_d = i_d + 1\n",
    "    i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b8deef69-9540-4695-a7f3-c8956831c114",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>corpus</th>\n",
       "      <th>doc_id</th>\n",
       "      <th>sent_id</th>\n",
       "      <th>eg_id</th>\n",
       "      <th>index</th>\n",
       "      <th>text</th>\n",
       "      <th>causal_text_w_pairs</th>\n",
       "      <th>num_rs</th>\n",
       "      <th>label</th>\n",
       "      <th>dup</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cnc</td>\n",
       "      <td>train_01_161</td>\n",
       "      <td>1462</td>\n",
       "      <td>0</td>\n",
       "      <td>cnc_train_01_161_1462_0</td>\n",
       "      <td>Two days before the attack , local police arre...</td>\n",
       "      <td>['Two days before the attack , &lt;ARG1&gt;local pol...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cnc</td>\n",
       "      <td>train_05_265</td>\n",
       "      <td>972</td>\n",
       "      <td>0</td>\n",
       "      <td>cnc_train_05_265_972_0</td>\n",
       "      <td>26th April 2014 09:26 AM Nearly three months a...</td>\n",
       "      <td>['26th April 2014 09:26 AM Nearly three months...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cnc</td>\n",
       "      <td>train_05_268</td>\n",
       "      <td>1656</td>\n",
       "      <td>0</td>\n",
       "      <td>cnc_train_05_268_1656_0</td>\n",
       "      <td>Subsequently , the police arrested six persons...</td>\n",
       "      <td>['Subsequently , &lt;ARG1&gt;the police arrested six...</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cnc</td>\n",
       "      <td>train_08_46</td>\n",
       "      <td>500</td>\n",
       "      <td>0</td>\n",
       "      <td>cnc_train_08_46_500_0</td>\n",
       "      <td>The police also arrested 79 persons at Suranda...</td>\n",
       "      <td>['&lt;ARG1&gt;The police also arrested 79 persons at...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cnc</td>\n",
       "      <td>train_03_183</td>\n",
       "      <td>3119</td>\n",
       "      <td>0</td>\n",
       "      <td>cnc_train_03_183_3119_0</td>\n",
       "      <td>Taking strong exception to the inordinate dela...</td>\n",
       "      <td>['&lt;ARG0&gt;&lt;SIG0&gt;Taking&lt;/SIG0&gt; strong exception t...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>cnc</td>\n",
       "      <td>train_05_136</td>\n",
       "      <td>1797</td>\n",
       "      <td>0</td>\n",
       "      <td>cnc_train_05_136_1797_0</td>\n",
       "      <td>He has been accused of using a slingshot durin...</td>\n",
       "      <td>['&lt;ARG0&gt;He has been accused of using a slingsh...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>cnc</td>\n",
       "      <td>train_06_179</td>\n",
       "      <td>1472</td>\n",
       "      <td>0</td>\n",
       "      <td>cnc_train_06_179_1472_0</td>\n",
       "      <td>15th September 2015 05:49 AM THIRUVANANTHAPURA...</td>\n",
       "      <td>['15th September 2015 05:49 AM THIRUVANANTHAPU...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>NEWS</td>\n",
       "      <td>train1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>prevent_our_data_0_02</td>\n",
       "      <td>Subcontractors  will  be offered a settlement ...</td>\n",
       "      <td>[\"Subcontractors  will  be offered a settlemen...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>cnc</td>\n",
       "      <td>train_07_205</td>\n",
       "      <td>1253</td>\n",
       "      <td>0</td>\n",
       "      <td>cnc_train_07_205_1253_0</td>\n",
       "      <td>September 22 , 2016 00:00 IST Ahmad writes to ...</td>\n",
       "      <td>['September 22 , 2016 00:00 IST Ahmad writes t...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>cnc</td>\n",
       "      <td>train_02_18</td>\n",
       "      <td>2121</td>\n",
       "      <td>0</td>\n",
       "      <td>cnc_train_02_18_2121_0</td>\n",
       "      <td>The traffic on Rohtak-Jhajjar highway remained...</td>\n",
       "      <td>['&lt;ARG0&gt;The traffic on Rohtak-Jhajjar highway ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>cnc</td>\n",
       "      <td>train_01_190</td>\n",
       "      <td>473</td>\n",
       "      <td>0</td>\n",
       "      <td>cnc_train_01_190_473_0</td>\n",
       "      <td>Tiruvannamalai : Cab drivers , owners take out...</td>\n",
       "      <td>['Tiruvannamalai : Cab drivers , owners take o...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>NEWS</td>\n",
       "      <td>train314</td>\n",
       "      <td>315</td>\n",
       "      <td>0</td>\n",
       "      <td>cause_our_data_0_0315</td>\n",
       "      <td>The reality is that there is more demand than ...</td>\n",
       "      <td>[\"The reality is that there is more demand tha...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   corpus        doc_id  sent_id  eg_id                    index  \\\n",
       "0     cnc  train_01_161     1462      0  cnc_train_01_161_1462_0   \n",
       "1     cnc  train_05_265      972      0   cnc_train_05_265_972_0   \n",
       "2     cnc  train_05_268     1656      0  cnc_train_05_268_1656_0   \n",
       "3     cnc   train_08_46      500      0    cnc_train_08_46_500_0   \n",
       "4     cnc  train_03_183     3119      0  cnc_train_03_183_3119_0   \n",
       "5     cnc  train_05_136     1797      0  cnc_train_05_136_1797_0   \n",
       "6     cnc  train_06_179     1472      0  cnc_train_06_179_1472_0   \n",
       "7    NEWS        train1        2      0    prevent_our_data_0_02   \n",
       "8     cnc  train_07_205     1253      0  cnc_train_07_205_1253_0   \n",
       "9     cnc   train_02_18     2121      0   cnc_train_02_18_2121_0   \n",
       "10    cnc  train_01_190      473      0   cnc_train_01_190_473_0   \n",
       "11   NEWS      train314      315      0    cause_our_data_0_0315   \n",
       "\n",
       "                                                 text  \\\n",
       "0   Two days before the attack , local police arre...   \n",
       "1   26th April 2014 09:26 AM Nearly three months a...   \n",
       "2   Subsequently , the police arrested six persons...   \n",
       "3   The police also arrested 79 persons at Suranda...   \n",
       "4   Taking strong exception to the inordinate dela...   \n",
       "5   He has been accused of using a slingshot durin...   \n",
       "6   15th September 2015 05:49 AM THIRUVANANTHAPURA...   \n",
       "7   Subcontractors  will  be offered a settlement ...   \n",
       "8   September 22 , 2016 00:00 IST Ahmad writes to ...   \n",
       "9   The traffic on Rohtak-Jhajjar highway remained...   \n",
       "10  Tiruvannamalai : Cab drivers , owners take out...   \n",
       "11  The reality is that there is more demand than ...   \n",
       "\n",
       "                                  causal_text_w_pairs  num_rs  label  dup  \n",
       "0   ['Two days before the attack , <ARG1>local pol...       1      1    0  \n",
       "1   ['26th April 2014 09:26 AM Nearly three months...       1      2    0  \n",
       "2   ['Subsequently , <ARG1>the police arrested six...       1      3    0  \n",
       "3   ['<ARG1>The police also arrested 79 persons at...       1      0    0  \n",
       "4   ['<ARG0><SIG0>Taking</SIG0> strong exception t...       1      0    0  \n",
       "5   ['<ARG0>He has been accused of using a slingsh...       1      0    0  \n",
       "6   ['15th September 2015 05:49 AM THIRUVANANTHAPU...       1      0    0  \n",
       "7   [\"Subcontractors  will  be offered a settlemen...       1      0    0  \n",
       "8   ['September 22 , 2016 00:00 IST Ahmad writes t...       2      0    0  \n",
       "9   ['<ARG0>The traffic on Rohtak-Jhajjar highway ...       1      0    0  \n",
       "10  ['Tiruvannamalai : Cab drivers , owners take o...       1      0    0  \n",
       "11  [\"The reality is that there is more demand tha...       1      0    0  "
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dup.head(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "862c1a4b-fed7-481d-ac69-c307de48e167",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>corpus</th>\n",
       "      <th>doc_id</th>\n",
       "      <th>sent_id</th>\n",
       "      <th>eg_id</th>\n",
       "      <th>index</th>\n",
       "      <th>text</th>\n",
       "      <th>causal_text_w_pairs</th>\n",
       "      <th>num_rs</th>\n",
       "      <th>label</th>\n",
       "      <th>dup</th>\n",
       "      <th>...</th>\n",
       "      <th>329</th>\n",
       "      <th>335</th>\n",
       "      <th>336</th>\n",
       "      <th>340</th>\n",
       "      <th>341</th>\n",
       "      <th>342</th>\n",
       "      <th>350</th>\n",
       "      <th>352</th>\n",
       "      <th>367</th>\n",
       "      <th>369</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cnc</td>\n",
       "      <td>train_01_161</td>\n",
       "      <td>1462.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>cnc_train_01_161_1462_0</td>\n",
       "      <td>Two days before the attack , local police arre...</td>\n",
       "      <td>['Two days before the attack , &lt;ARG1&gt;local pol...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cnc</td>\n",
       "      <td>train_05_265</td>\n",
       "      <td>972.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>cnc_train_05_265_972_0</td>\n",
       "      <td>26th April 2014 09:26 AM Nearly three months a...</td>\n",
       "      <td>['26th April 2014 09:26 AM Nearly three months...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cnc</td>\n",
       "      <td>train_05_268</td>\n",
       "      <td>1656.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>cnc_train_05_268_1656_0</td>\n",
       "      <td>Subsequently , the police arrested six persons...</td>\n",
       "      <td>['Subsequently , &lt;ARG1&gt;the police arrested six...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cnc</td>\n",
       "      <td>train_08_46</td>\n",
       "      <td>500.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>cnc_train_08_46_500_0</td>\n",
       "      <td>The police also arrested 79 persons at Suranda...</td>\n",
       "      <td>['&lt;ARG1&gt;The police also arrested 79 persons at...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cnc</td>\n",
       "      <td>train_03_183</td>\n",
       "      <td>3119.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>cnc_train_03_183_3119_0</td>\n",
       "      <td>Taking strong exception to the inordinate dela...</td>\n",
       "      <td>['&lt;ARG0&gt;&lt;SIG0&gt;Taking&lt;/SIG0&gt; strong exception t...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 102 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  corpus        doc_id  sent_id  eg_id                    index  \\\n",
       "0    cnc  train_01_161   1462.0    0.0  cnc_train_01_161_1462_0   \n",
       "1    cnc  train_05_265    972.0    0.0   cnc_train_05_265_972_0   \n",
       "2    cnc  train_05_268   1656.0    0.0  cnc_train_05_268_1656_0   \n",
       "3    cnc   train_08_46    500.0    0.0    cnc_train_08_46_500_0   \n",
       "4    cnc  train_03_183   3119.0    0.0  cnc_train_03_183_3119_0   \n",
       "\n",
       "                                                text  \\\n",
       "0  Two days before the attack , local police arre...   \n",
       "1  26th April 2014 09:26 AM Nearly three months a...   \n",
       "2  Subsequently , the police arrested six persons...   \n",
       "3  The police also arrested 79 persons at Suranda...   \n",
       "4  Taking strong exception to the inordinate dela...   \n",
       "\n",
       "                                 causal_text_w_pairs  num_rs  label  dup  ...  \\\n",
       "0  ['Two days before the attack , <ARG1>local pol...     1.0    1.0    0  ...   \n",
       "1  ['26th April 2014 09:26 AM Nearly three months...     1.0    2.0    0  ...   \n",
       "2  ['Subsequently , <ARG1>the police arrested six...     1.0    3.0    0  ...   \n",
       "3  ['<ARG1>The police also arrested 79 persons at...     1.0    0.0    0  ...   \n",
       "4  ['<ARG0><SIG0>Taking</SIG0> strong exception t...     1.0    0.0    0  ...   \n",
       "\n",
       "   329  335  336  340  341  342  350  352  367  369  \n",
       "0  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
       "1  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
       "2  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
       "3  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
       "4  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
       "\n",
       "[5 rows x 102 columns]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_before.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "98c1847e-9115-49f4-81d4-a3c0da9f5c96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>corpus</th>\n",
       "      <th>doc_id</th>\n",
       "      <th>sent_id</th>\n",
       "      <th>eg_id</th>\n",
       "      <th>index</th>\n",
       "      <th>text</th>\n",
       "      <th>causal_text_w_pairs</th>\n",
       "      <th>num_rs</th>\n",
       "      <th>label</th>\n",
       "      <th>dup</th>\n",
       "      <th>...</th>\n",
       "      <th>329</th>\n",
       "      <th>335</th>\n",
       "      <th>336</th>\n",
       "      <th>340</th>\n",
       "      <th>341</th>\n",
       "      <th>342</th>\n",
       "      <th>350</th>\n",
       "      <th>352</th>\n",
       "      <th>367</th>\n",
       "      <th>369</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>580</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>train_03_240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>581</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>582</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>583</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cnc_train_03_240_1030_0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>584</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Earlier , in a letter to the HRD ministry thro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 102 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    corpus doc_id  sent_id  eg_id index text causal_text_w_pairs  num_rs  \\\n",
       "580    NaN    NaN      NaN    NaN   NaN  NaN                 NaN     NaN   \n",
       "581    NaN    NaN      NaN    NaN   NaN  NaN                 NaN     NaN   \n",
       "582    NaN    NaN      NaN    NaN   NaN  NaN                 NaN     NaN   \n",
       "583    NaN    NaN      NaN    NaN   NaN  NaN                 NaN     NaN   \n",
       "584    NaN    NaN      NaN    NaN   NaN  NaN                 NaN     NaN   \n",
       "\n",
       "     label  dup  ...  329  335  336  340  341  342  350  352  367  \\\n",
       "580    NaN    2  ...  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN   \n",
       "581    NaN    2  ...  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN   \n",
       "582    NaN    2  ...  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN   \n",
       "583    NaN    2  ...  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN   \n",
       "584    NaN    2  ...  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN   \n",
       "\n",
       "                                                   369  \n",
       "580                                       train_03_240  \n",
       "581                                               1030  \n",
       "582                                                  0  \n",
       "583                            cnc_train_03_240_1030_0  \n",
       "584  Earlier , in a letter to the HRD ministry thro...  \n",
       "\n",
       "[5 rows x 102 columns]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_after.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "32fa553c-9de0-4eff-82b4-0cef98a0096e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "tt = pd.DataFrame(df_dup.iloc[:4+1, :])\n",
    "tt.head()\n",
    "print(len(tt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "112e4bfc-b273-47c8-9d6f-2688db8d384f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "373\n"
     ]
    }
   ],
   "source": [
    "aa =  pd.DataFrame(df_dup.iloc[4+1:, :])\n",
    "aa.head()\n",
    "print(len(aa))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2d7b529c-6f82-4dd0-a3c3-bd84f9cb2b4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "378\n"
     ]
    }
   ],
   "source": [
    "xx = pd.concat([tt, aa]).reset_index(drop=True)\n",
    "xx.head()\n",
    "print(len(xx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f082ae33-a06d-4bf0-a8a6-3ec9bdcfe127",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>corpus</th>\n",
       "      <th>doc_id</th>\n",
       "      <th>sent_id</th>\n",
       "      <th>eg_id</th>\n",
       "      <th>index</th>\n",
       "      <th>text</th>\n",
       "      <th>causal_text_w_pairs</th>\n",
       "      <th>num_rs</th>\n",
       "      <th>label</th>\n",
       "      <th>dup</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>cnc</td>\n",
       "      <td>train_07_205</td>\n",
       "      <td>1253</td>\n",
       "      <td>0</td>\n",
       "      <td>cnc_train_07_205_1253_0</td>\n",
       "      <td>September 22 , 2016 00:00 IST Ahmad writes to ...</td>\n",
       "      <td>['September 22 , 2016 00:00 IST Ahmad writes t...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  corpus        doc_id  sent_id  eg_id                    index  \\\n",
       "8    cnc  train_07_205     1253      0  cnc_train_07_205_1253_0   \n",
       "\n",
       "                                                text  \\\n",
       "8  September 22 , 2016 00:00 IST Ahmad writes to ...   \n",
       "\n",
       "                                 causal_text_w_pairs  num_rs  label  dup  \n",
       "8  ['September 22 , 2016 00:00 IST Ahmad writes t...       2      0    0  "
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = df.loc[[8],:]\n",
    "q.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "e8111f0c-9575-45ed-bf35-1e254006c859",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>corpus</th>\n",
       "      <th>doc_id</th>\n",
       "      <th>sent_id</th>\n",
       "      <th>eg_id</th>\n",
       "      <th>index</th>\n",
       "      <th>text</th>\n",
       "      <th>causal_text_w_pairs</th>\n",
       "      <th>num_rs</th>\n",
       "      <th>label</th>\n",
       "      <th>dup</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cnc</td>\n",
       "      <td>train_01_161</td>\n",
       "      <td>1462</td>\n",
       "      <td>0</td>\n",
       "      <td>cnc_train_01_161_1462_0</td>\n",
       "      <td>Two days before the attack , local police arre...</td>\n",
       "      <td>['Two days before the attack , &lt;ARG1&gt;local pol...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cnc</td>\n",
       "      <td>train_05_265</td>\n",
       "      <td>972</td>\n",
       "      <td>0</td>\n",
       "      <td>cnc_train_05_265_972_0</td>\n",
       "      <td>26th April 2014 09:26 AM Nearly three months a...</td>\n",
       "      <td>['26th April 2014 09:26 AM Nearly three months...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cnc</td>\n",
       "      <td>train_05_268</td>\n",
       "      <td>1656</td>\n",
       "      <td>0</td>\n",
       "      <td>cnc_train_05_268_1656_0</td>\n",
       "      <td>Subsequently , the police arrested six persons...</td>\n",
       "      <td>['Subsequently , &lt;ARG1&gt;the police arrested six...</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cnc</td>\n",
       "      <td>train_08_46</td>\n",
       "      <td>500</td>\n",
       "      <td>0</td>\n",
       "      <td>cnc_train_08_46_500_0</td>\n",
       "      <td>The police also arrested 79 persons at Suranda...</td>\n",
       "      <td>['&lt;ARG1&gt;The police also arrested 79 persons at...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cnc</td>\n",
       "      <td>train_03_183</td>\n",
       "      <td>3119</td>\n",
       "      <td>0</td>\n",
       "      <td>cnc_train_03_183_3119_0</td>\n",
       "      <td>Taking strong exception to the inordinate dela...</td>\n",
       "      <td>['&lt;ARG0&gt;&lt;SIG0&gt;Taking&lt;/SIG0&gt; strong exception t...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  corpus        doc_id  sent_id  eg_id                    index  \\\n",
       "0    cnc  train_01_161     1462      0  cnc_train_01_161_1462_0   \n",
       "1    cnc  train_05_265      972      0   cnc_train_05_265_972_0   \n",
       "2    cnc  train_05_268     1656      0  cnc_train_05_268_1656_0   \n",
       "3    cnc   train_08_46      500      0    cnc_train_08_46_500_0   \n",
       "4    cnc  train_03_183     3119      0  cnc_train_03_183_3119_0   \n",
       "\n",
       "                                                text  \\\n",
       "0  Two days before the attack , local police arre...   \n",
       "1  26th April 2014 09:26 AM Nearly three months a...   \n",
       "2  Subsequently , the police arrested six persons...   \n",
       "3  The police also arrested 79 persons at Suranda...   \n",
       "4  Taking strong exception to the inordinate dela...   \n",
       "\n",
       "                                 causal_text_w_pairs  num_rs  label  dup  \n",
       "0  ['Two days before the attack , <ARG1>local pol...       1      1    0  \n",
       "1  ['26th April 2014 09:26 AM Nearly three months...       1      2    0  \n",
       "2  ['Subsequently , <ARG1>the police arrested six...       1      3    0  \n",
       "3  ['<ARG1>The police also arrested 79 persons at...       1      0    0  \n",
       "4  ['<ARG0><SIG0>Taking</SIG0> strong exception t...       1      0    0  "
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zz = pd.concat([tt, q, aa]).reset_index(drop=True)\n",
    "zz.head()\n",
    "#print(len(zz))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "1f71ab97-74cc-4498-87b5-b56d4afc2204",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context</th>\n",
       "      <th>triplets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Crackers , drumbeats welcome Rahul ascendancy ...</td>\n",
       "      <td>&lt;triplet&gt; Hundreds of Youth Congress and NSUI ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Two days before the attack , local police arre...</td>\n",
       "      <td>&lt;triplet&gt; they tried to illegally cross into V...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>26th April 2014 09:26 AM Nearly three months a...</td>\n",
       "      <td>&lt;triplet&gt; demanding that the police arrest tho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Subsequently , the police arrested six persons...</td>\n",
       "      <td>&lt;triplet&gt; in connection with the murder and ei...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>In a statement prepared by lawyers for the uni...</td>\n",
       "      <td>&lt;triplet&gt; reaching &lt;subj&gt; agreed &lt;obj&gt; cause</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             context  \\\n",
       "0  Crackers , drumbeats welcome Rahul ascendancy ...   \n",
       "1  Two days before the attack , local police arre...   \n",
       "2  26th April 2014 09:26 AM Nearly three months a...   \n",
       "3  Subsequently , the police arrested six persons...   \n",
       "4  In a statement prepared by lawyers for the uni...   \n",
       "\n",
       "                                            triplets  \n",
       "0  <triplet> Hundreds of Youth Congress and NSUI ...  \n",
       "1  <triplet> they tried to illegally cross into V...  \n",
       "2  <triplet> demanding that the police arrest tho...  \n",
       "3  <triplet> in connection with the murder and ei...  \n",
       "4       <triplet> reaching <subj> agreed <obj> cause  "
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ddf = pd.read_csv('test.csv')\n",
    "ddf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "975d685b-df11-4ec7-a09d-450704e27766",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf2 = ddf['context']\n",
    "ddf2 = pd.DataFrame(ddf2)\n",
    "ddf2.head()\n",
    "ddf2.to_csv('testing.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "696c1a98-d5e5-4522-8a62-f5d0c210964c",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = '<triplet> reaching <subj> agreed <obj> cause'\n",
    "ddf2['triplets'] = s\n",
    "ddf2.to_csv('testing2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "69c8c798-c3d9-4471-a936-6e4d3b5cf660",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rebel = pd.read_csv('rebel_prediction/pred_rebel.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "c1d064d3-4ed9-4597-bb93-ff00664d37af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>subject</th>\n",
       "      <th>relation</th>\n",
       "      <th>object</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Crackers, drumbeats welcome Rahul ascendancy t...</td>\n",
       "      <td>welcomed</td>\n",
       "      <td>cause</td>\n",
       "      <td>bursting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Two days before the attack, local police arres...</td>\n",
       "      <td>arrested</td>\n",
       "      <td>cause</td>\n",
       "      <td>illegally cross</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>26th April 2014 09:26 AM Nearly three months a...</td>\n",
       "      <td>was</td>\n",
       "      <td>cause</td>\n",
       "      <td>hunger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Subsequently, the police arrested six persons ...</td>\n",
       "      <td>arrested</td>\n",
       "      <td>cause</td>\n",
       "      <td>turned</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>In a statement prepared by lawyers for the uni...</td>\n",
       "      <td>agreement</td>\n",
       "      <td>prevent</td>\n",
       "      <td>judgment</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text    subject relation  \\\n",
       "0  Crackers, drumbeats welcome Rahul ascendancy t...   welcomed    cause   \n",
       "1  Two days before the attack, local police arres...   arrested    cause   \n",
       "2  26th April 2014 09:26 AM Nearly three months a...        was    cause   \n",
       "3  Subsequently, the police arrested six persons ...   arrested    cause   \n",
       "4  In a statement prepared by lawyers for the uni...  agreement  prevent   \n",
       "\n",
       "            object  \n",
       "0         bursting  \n",
       "1  illegally cross  \n",
       "2           hunger  \n",
       "3           turned  \n",
       "4         judgment  "
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_rebel.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "313ea74a-0ec3-428d-870e-18ea17addbac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>subject</th>\n",
       "      <th>relation</th>\n",
       "      <th>object</th>\n",
       "      <th>triplet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Crackers, drumbeats welcome Rahul ascendancy t...</td>\n",
       "      <td>welcomed</td>\n",
       "      <td>cause</td>\n",
       "      <td>bursting</td>\n",
       "      <td>[welcomed, cause, bursting]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Two days before the attack, local police arres...</td>\n",
       "      <td>arrested</td>\n",
       "      <td>cause</td>\n",
       "      <td>illegally cross</td>\n",
       "      <td>[arrested, cause, illegally cross]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>26th April 2014 09:26 AM Nearly three months a...</td>\n",
       "      <td>was</td>\n",
       "      <td>cause</td>\n",
       "      <td>hunger</td>\n",
       "      <td>[was, cause, hunger]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Subsequently, the police arrested six persons ...</td>\n",
       "      <td>arrested</td>\n",
       "      <td>cause</td>\n",
       "      <td>turned</td>\n",
       "      <td>[arrested, cause, turned]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>In a statement prepared by lawyers for the uni...</td>\n",
       "      <td>agreement</td>\n",
       "      <td>prevent</td>\n",
       "      <td>judgment</td>\n",
       "      <td>[agreement, prevent, judgment]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text    subject relation  \\\n",
       "0  Crackers, drumbeats welcome Rahul ascendancy t...   welcomed    cause   \n",
       "1  Two days before the attack, local police arres...   arrested    cause   \n",
       "2  26th April 2014 09:26 AM Nearly three months a...        was    cause   \n",
       "3  Subsequently, the police arrested six persons ...   arrested    cause   \n",
       "4  In a statement prepared by lawyers for the uni...  agreement  prevent   \n",
       "\n",
       "            object                             triplet  \n",
       "0         bursting         [welcomed, cause, bursting]  \n",
       "1  illegally cross  [arrested, cause, illegally cross]  \n",
       "2           hunger                [was, cause, hunger]  \n",
       "3           turned           [arrested, cause, turned]  \n",
       "4         judgment      [agreement, prevent, judgment]  "
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_rebel['triplet'] = df_rebel.apply(lambda row: [row['subject'], row['relation'], row['object']], axis=1)\n",
    "df_rebel.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "08d7c76c-bdeb-43e3-ae21-fdc726b6f2c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>here are some test sentences.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I need to see if this works.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hello hello 3.5.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             text\n",
       "0  here are some test sentences. \n",
       "1   I need to see if this works. \n",
       "2               hello hello 3.5. "
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = 'here are some test sentences. I need to see if this works. hello hello 3.5'\n",
    "parts = s.split('. ')\n",
    "parts = [part + '. ' for part in parts if part != '']\n",
    "df_s = pd.DataFrame()\n",
    "df_s['text'] = parts\n",
    "df_s.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "b400eee4-51e9-48fe-b8ce-65d38249739b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gustavoflores/miniconda3/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "usage: ipykernel_launcher.py [-h] [--dataset_name DATASET_NAME]\n",
      "                             [--dataset_config_name DATASET_CONFIG_NAME]\n",
      "                             [--train_file TRAIN_FILE]\n",
      "                             [--validation_file VALIDATION_FILE]\n",
      "                             [--test_file TEST_FILE]\n",
      "                             [--max_train_samples MAX_TRAIN_SAMPLES]\n",
      "                             [--max_eval_samples MAX_EVAL_SAMPLES]\n",
      "                             [--max_test_samples MAX_TEST_SAMPLES]\n",
      "                             [--max_length MAX_LENGTH] [--pad_to_max_length]\n",
      "                             [--model_name_or_path MODEL_NAME_OR_PATH]\n",
      "                             [--config_name CONFIG_NAME]\n",
      "                             [--tokenizer_name TOKENIZER_NAME]\n",
      "                             [--per_device_train_batch_size PER_DEVICE_TRAIN_BATCH_SIZE]\n",
      "                             [--per_device_eval_batch_size PER_DEVICE_EVAL_BATCH_SIZE]\n",
      "                             [--per_device_test_batch_size PER_DEVICE_TEST_BATCH_SIZE]\n",
      "                             [--learning_rate LEARNING_RATE]\n",
      "                             [--weight_decay WEIGHT_DECAY]\n",
      "                             [--num_train_epochs NUM_TRAIN_EPOCHS]\n",
      "                             [--max_train_steps MAX_TRAIN_STEPS]\n",
      "                             [--gradient_accumulation_steps GRADIENT_ACCUMULATION_STEPS]\n",
      "                             [--lr_scheduler_type {linear,cosine,cosine_with_restarts,polynomial,constant,constant_with_warmup}]\n",
      "                             [--num_warmup_steps NUM_WARMUP_STEPS]\n",
      "                             [--output_dir OUTPUT_DIR] [--seed SEED]\n",
      "                             [--model_type {albert,align,altclip,audio-spectrogram-transformer,autoformer,bark,bart,beit,bert,bert-generation,big_bird,bigbird_pegasus,biogpt,bit,blenderbot,blenderbot-small,blip,blip-2,bloom,bridgetower,bros,camembert,canine,chinese_clip,chinese_clip_vision_model,clap,clip,clip_vision_model,clipseg,clvp,llama,codegen,cohere,conditional_detr,convbert,convnext,convnextv2,cpmant,ctrl,cvt,data2vec-audio,data2vec-text,data2vec-vision,dbrx,deberta,deberta-v2,decision_transformer,deformable_detr,deit,deta,detr,dinat,dinov2,distilbert,donut-swin,dpr,dpt,efficientformer,efficientnet,electra,encodec,ernie,ernie_m,esm,falcon,fastspeech2_conformer,flaubert,flava,fnet,focalnet,fsmt,funnel,gemma,gemma2,git,glpn,gpt2,gpt2,gpt_bigcode,gpt_neo,gpt_neox,gpt_neox_japanese,gptj,gptsan-japanese,graphormer,grounding-dino,groupvit,hubert,ibert,idefics,idefics2,imagegpt,informer,jamba,jetmoe,jukebox,kosmos-2,layoutlm,layoutlmv2,layoutlmv3,led,levit,lilt,llama,longformer,longt5,luke,lxmert,m2m_100,mamba,marian,markuplm,mask2former,maskformer,maskformer-swin,mbart,mctct,mega,megatron-bert,mgp-str,mistral,mixtral,mobilebert,mobilenet_v1,mobilenet_v2,mobilevit,mobilevitv2,mpnet,mpt,mra,mt5,musicgen,musicgen_melody,mvp,nat,nezha,nllb-moe,nystromformer,olmo,oneformer,open-llama,openai-gpt,opt,owlv2,owlvit,patchtsmixer,patchtst,pegasus,pegasus_x,perceiver,persimmon,phi,phi3,plbart,poolformer,prophetnet,pvt,pvt_v2,qdqbert,qwen2,qwen2_moe,recurrent_gemma,reformer,regnet,rembert,resnet,retribert,roberta,roberta-prelayernorm,roc_bert,roformer,rt_detr,rwkv,sam,seamless_m4t,seamless_m4t_v2,segformer,seggpt,sew,sew-d,siglip,siglip_vision_model,speech_to_text,speecht5,splinter,squeezebert,stablelm,starcoder2,swiftformer,swin,swin2sr,swinv2,switch_transformers,t5,table-transformer,tapas,time_series_transformer,timesformer,timm_backbone,trajectory_transformer,transfo-xl,tvlt,tvp,udop,umt5,unispeech,unispeech-sat,univnet,van,videomae,vilt,vision-text-dual-encoder,visual_bert,vit,vit-hybrid,vit_mae,vit_msn,vitdet,vits,vivit,wav2vec2,wav2vec2-bert,wav2vec2-conformer,wavlm,whisper,xclip,xglm,xlm,xlm-prophetnet,xlm-roberta,xlm-roberta-xl,xlnet,xmod,yolos,yoso}]\n",
      "                             [--label_all_tokens]\n",
      "                             [--return_entity_level_metrics]\n",
      "                             [--task_name {ner,pos,chunk}] [--debug]\n",
      "                             [--push_to_hub] [--hub_model_id HUB_MODEL_ID]\n",
      "                             [--hub_token HUB_TOKEN]\n",
      "                             [--checkpointing_steps CHECKPOINTING_STEPS]\n",
      "                             [--resume_from_checkpoint RESUME_FROM_CHECKPOINT]\n",
      "                             [--with_tracking] [--report_to REPORT_TO]\n",
      "                             [--ignore_mismatched_sizes] [--add_signal_bias]\n",
      "                             [--signal_bias_on_top_of_lm]\n",
      "                             [--postprocessing_position_selector] [--mlp]\n",
      "                             [--signal_classification]\n",
      "                             [--pretrained_signal_detector]\n",
      "                             [--signal_model_and_tokenizer_path SIGNAL_MODEL_AND_TOKENIZER_PATH]\n",
      "                             [--beam_search] [--dropout DROPOUT]\n",
      "                             [--use_best_model]\n",
      "                             [--load_checkpoint_for_test LOAD_CHECKPOINT_FOR_TEST]\n",
      "                             [--do_train] [--do_test]\n",
      "                             [--augmentation_file AUGMENTATION_FILE]\n",
      "                             [--topk TOPK] [--use_cpu]\n",
      "                             [--rebel_inf_model_name_or_path REBEL_INF_MODEL_NAME_OR_PATH]\n",
      "                             [--st1_do_predict] [--st1_use_cpu]\n",
      "                             [--st1_output_dir ST1_OUTPUT_DIR]\n",
      "                             [--st1_task_name ST1_TASK_NAME]\n",
      "                             [--st1_dataset_name ST1_DATASET_NAME]\n",
      "                             [--st1_dataset_config_name ST1_DATASET_CONFIG_NAME]\n",
      "                             [--st1_max_seq_length ST1_MAX_SEQ_LENGTH]\n",
      "                             [--st1_overwrite_cache] [--st1_pad_to_max_length]\n",
      "                             [--st1_max_train_samples ST1_MAX_TRAIN_SAMPLES]\n",
      "                             [--st1_max_eval_samples ST1_MAX_EVAL_SAMPLES]\n",
      "                             [--st1_max_predict_samples ST1_MAX_PREDICT_SAMPLES]\n",
      "                             [--st1_model_name_or_path ST1_MODEL_NAME_OR_PATH]\n",
      "                             [--st1_config_name ST1_CONFIG_NAME]\n",
      "                             [--st1_tokenizer_name ST1_TOKENIZER_NAME]\n",
      "                             [--st1_cache_dir ST1_CACHE_DIR]\n",
      "                             [--st1_use_fast_tokenizer]\n",
      "                             [--st1_model_revision ST1_MODEL_REVISION]\n",
      "                             [--st1_use_auth_token]\n",
      "                             [--st1_train_file ST1_TRAIN_FILE]\n",
      "                             [--st1_validation_file ST1_VALIDATION_FILE]\n",
      "                             [--st1_test_file ST1_TEST_FILE]\n",
      "                             [--st1_is_regression] [--st1_seed ST1_SEED]\n",
      "                             [--st2_pretrained_path ST2_PRETRAINED_PATH]\n",
      "                             [--st2_dataset_name ST2_DATASET_NAME]\n",
      "                             [--st2_dataset_config_name ST2_DATASET_CONFIG_NAME]\n",
      "                             [--st2_train_file ST2_TRAIN_FILE]\n",
      "                             [--st2_validation_file ST2_VALIDATION_FILE]\n",
      "                             [--st2_test_file ST2_TEST_FILE]\n",
      "                             [--st2_max_train_samples ST2_MAX_TRAIN_SAMPLES]\n",
      "                             [--st2_max_eval_samples ST2_MAX_EVAL_SAMPLES]\n",
      "                             [--st2_max_test_samples ST2_MAX_TEST_SAMPLES]\n",
      "                             [--st2_max_length ST2_MAX_LENGTH]\n",
      "                             [--st2_pad_to_max_length]\n",
      "                             [--st2_model_name_or_path ST2_MODEL_NAME_OR_PATH]\n",
      "                             [--st2_config_name ST2_CONFIG_NAME]\n",
      "                             [--st2_tokenizer_name ST2_TOKENIZER_NAME]\n",
      "                             [--st2_per_device_train_batch_size ST2_PER_DEVICE_TRAIN_BATCH_SIZE]\n",
      "                             [--st2_per_device_eval_batch_size ST2_PER_DEVICE_EVAL_BATCH_SIZE]\n",
      "                             [--st2_per_device_test_batch_size ST2_PER_DEVICE_TEST_BATCH_SIZE]\n",
      "                             [--st2_learning_rate ST2_LEARNING_RATE]\n",
      "                             [--st2_weight_decay ST2_WEIGHT_DECAY]\n",
      "                             [--st2_num_train_epochs ST2_NUM_TRAIN_EPOCHS]\n",
      "                             [--st2_max_train_steps ST2_MAX_TRAIN_STEPS]\n",
      "                             [--st2_gradient_accumulation_steps ST2_GRADIENT_ACCUMULATION_STEPS]\n",
      "                             [--st2_lr_scheduler_type {linear,cosine,cosine_with_restarts,polynomial,constant,constant_with_warmup}]\n",
      "                             [--st2_num_warmup_steps ST2_NUM_WARMUP_STEPS]\n",
      "                             [--st2_output_dir ST2_OUTPUT_DIR]\n",
      "                             [--st2_seed ST2_SEED]\n",
      "                             [--st2_model_type {albert,align,altclip,audio-spectrogram-transformer,autoformer,bark,bart,beit,bert,bert-generation,big_bird,bigbird_pegasus,biogpt,bit,blenderbot,blenderbot-small,blip,blip-2,bloom,bridgetower,bros,camembert,canine,chinese_clip,chinese_clip_vision_model,clap,clip,clip_vision_model,clipseg,clvp,llama,codegen,cohere,conditional_detr,convbert,convnext,convnextv2,cpmant,ctrl,cvt,data2vec-audio,data2vec-text,data2vec-vision,dbrx,deberta,deberta-v2,decision_transformer,deformable_detr,deit,deta,detr,dinat,dinov2,distilbert,donut-swin,dpr,dpt,efficientformer,efficientnet,electra,encodec,ernie,ernie_m,esm,falcon,fastspeech2_conformer,flaubert,flava,fnet,focalnet,fsmt,funnel,gemma,gemma2,git,glpn,gpt2,gpt2,gpt_bigcode,gpt_neo,gpt_neox,gpt_neox_japanese,gptj,gptsan-japanese,graphormer,grounding-dino,groupvit,hubert,ibert,idefics,idefics2,imagegpt,informer,jamba,jetmoe,jukebox,kosmos-2,layoutlm,layoutlmv2,layoutlmv3,led,levit,lilt,llama,longformer,longt5,luke,lxmert,m2m_100,mamba,marian,markuplm,mask2former,maskformer,maskformer-swin,mbart,mctct,mega,megatron-bert,mgp-str,mistral,mixtral,mobilebert,mobilenet_v1,mobilenet_v2,mobilevit,mobilevitv2,mpnet,mpt,mra,mt5,musicgen,musicgen_melody,mvp,nat,nezha,nllb-moe,nystromformer,olmo,oneformer,open-llama,openai-gpt,opt,owlv2,owlvit,patchtsmixer,patchtst,pegasus,pegasus_x,perceiver,persimmon,phi,phi3,plbart,poolformer,prophetnet,pvt,pvt_v2,qdqbert,qwen2,qwen2_moe,recurrent_gemma,reformer,regnet,rembert,resnet,retribert,roberta,roberta-prelayernorm,roc_bert,roformer,rt_detr,rwkv,sam,seamless_m4t,seamless_m4t_v2,segformer,seggpt,sew,sew-d,siglip,siglip_vision_model,speech_to_text,speecht5,splinter,squeezebert,stablelm,starcoder2,swiftformer,swin,swin2sr,swinv2,switch_transformers,t5,table-transformer,tapas,time_series_transformer,timesformer,timm_backbone,trajectory_transformer,transfo-xl,tvlt,tvp,udop,umt5,unispeech,unispeech-sat,univnet,van,videomae,vilt,vision-text-dual-encoder,visual_bert,vit,vit-hybrid,vit_mae,vit_msn,vitdet,vits,vivit,wav2vec2,wav2vec2-bert,wav2vec2-conformer,wavlm,whisper,xclip,xglm,xlm,xlm-prophetnet,xlm-roberta,xlm-roberta-xl,xlnet,xmod,yolos,yoso}]\n",
      "                             [--st2_label_all_tokens]\n",
      "                             [--st2_return_entity_level_metrics]\n",
      "                             [--st2_task_name {ner,pos,chunk}] [--st2_debug]\n",
      "                             [--st2_push_to_hub]\n",
      "                             [--st2_hub_model_id ST2_HUB_MODEL_ID]\n",
      "                             [--st2_hub_token ST2_HUB_TOKEN]\n",
      "                             [--st2_checkpointing_steps ST2_CHECKPOINTING_STEPS]\n",
      "                             [--st2_resume_from_checkpoint ST2_RESUME_FROM_CHECKPOINT]\n",
      "                             [--st2_with_tracking]\n",
      "                             [--st2_report_to ST2_REPORT_TO]\n",
      "                             [--st2_ignore_mismatched_sizes]\n",
      "                             [--st2_add_signal_bias]\n",
      "                             [--st2_signal_bias_on_top_of_lm]\n",
      "                             [--st2_postprocessing_position_selector]\n",
      "                             [--st2_mlp] [--st2_signal_classification]\n",
      "                             [--st2_pretrained_signal_detector]\n",
      "                             [--st2_signal_model_and_tokenizer_path ST2_SIGNAL_MODEL_AND_TOKENIZER_PATH]\n",
      "                             [--st2_beam_search] [--st2_dropout ST2_DROPOUT]\n",
      "                             [--st2_use_best_model]\n",
      "                             [--st2_load_checkpoint_for_test ST2_LOAD_CHECKPOINT_FOR_TEST]\n",
      "                             [--st2_do_train] [--st2_do_test]\n",
      "                             [--st2_augmentation_file ST2_AUGMENTATION_FILE]\n",
      "                             [--st2_topk ST2_TOPK]\n",
      "                             [--filter_train_file FILTER_TRAIN_FILE]\n",
      "                             [--filter_val_file FILTER_VAL_FILE]\n",
      "                             [--filter_test_file FILTER_TEST_FILE]\n",
      "                             [--filter_threshold FILTER_THRESHOLD]\n",
      "                             [--filter_model_path FILTER_MODEL_PATH]\n",
      "                             [--LLMS_task {test}]\n",
      "                             [--LLMS_news_dataset LLMS_NEWS_DATASET]\n",
      "                             [--LLMS_test_dataset LLMS_TEST_DATASET]\n",
      "                             [--LLMS_num_examples LLMS_NUM_EXAMPLES]\n",
      "                             [--LLMS_llm {zephyr,dpo,una,solar,gpt4}]\n",
      "                             [--LLMS_template LLMS_TEMPLATE]\n",
      "                             [--LLMS_output LLMS_OUTPUT]\n",
      "                             [--LLMS_api_key LLMS_API_KEY] [--LLMS_verbose]\n",
      "                             [--LLMS_log {10,20,30,40,50}] [--st2_flag]\n",
      "                             [--st1_flag] [--rebel_flag] [--LLM_flag]\n",
      "                             [--subtask1_flag] [--subtask2_flag]\n",
      "                             [--subtask3_flag] [--config_file CONFIG_FILE]\n",
      "                             [--text_from_user TEXT_FROM_USER]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f /Users/gustavoflores/Library/Jupyter/runtime/kernel-41b0fff9-6d29-4814-b64a-79f5c388b368.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gustavoflores/miniconda3/lib/python3.12/site-packages/IPython/core/interactiveshell.py:3585: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "from two_step_model import run_pipeline\n",
    "j = run_pipeline('config.ini')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "b3a07093-e1fd-489f-8540-aa672a9395f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "config['DEFAULT'] = {\n",
    "    'text_from_user': (\n",
    "        \"here are some test sentences. I need to see if this works. hello hello 3.5. \"\n",
    "        \"help me see if this works. The responses should be a mess. I need to figure out the right amount of sentences in order to let the program run. \"\n",
    "        \"help me with the process. these are words and multiple words form a sentence. sentences form paragraphs. \"\n",
    "        \"I was distracted so I fell. when I hit the ground I hurt myself. when you water a plant in the future it will grow. \"\n",
    "        \"I was sad so I listened to music. I need more samples so I create more samples. Dropping the cup made me frustrated. \"\n",
    "        \"Pushing the domino made it fall. The wind blowing made the house of cards fall. After dieting I lost weight. \"\n",
    "        \"I was late because the strikes happened. seeing this caused me to be sad. watching food videos made me hungry. \"\n",
    "        \"I was thirsty so I drank water.\"\n",
    "    )\n",
    "}\n",
    "\n",
    "with open('config2.ini', 'w') as configfile:\n",
    "    config.write(configfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "93bd3e58-b4e2-4ce4-95fb-8eec072794b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "openapi: 3.0.3\n",
      "info:\n",
      "  title: Relation Detection - OpenAPI 3.3\n",
      "  description: Here we provide a platform to run and compare multiple models against each other\n",
      "  version: 1.0.11\n",
      "paths:\n",
      "  \n",
      "  /Flag_List:\n",
      "    get:\n",
      "      summary: Get items with individual choices\n",
      "      parameters:\n",
      "        - in: query\n",
      "          name: ut\n",
      "          required: false\n",
      "          schema:\n",
      "            type: string\n",
      "          description: Type here if you want to input your own sentences; please seperate each sentence with a period followed by a space.\n",
      "        - in: query\n",
      "          name: st1\n",
      "          required: false\n",
      "          schema:\n",
      "            type: string\n",
      "            default: roberta_st1_best_model\n",
      "          description: Determines which model should be used for subtask 1 if subtask 1 is being done\n",
      "        - in: query\n",
      "          name: st2\n",
      "          required: false\n",
      "          schema:\n",
      "            type: string\n",
      "            default: roberta_st2_epoch_9\n",
      "          description: Determines which model should be used for subtask 2 if subtask 2 is being done\n",
      "        - in: query\n",
      "          name: st3\n",
      "          required: false\n",
      "          schema:\n",
      "            type: string\n",
      "            default: rebel_st3_model_our_data_gpt_augmented.pth\n",
      "            enum:\n",
      "              - rebel\n",
      "              - gpt\n",
      "              - none\n",
      "          description: Determines which model should be used for subtask 3 if subtask 3 is being done\n",
      "        - in: query\n",
      "          name: api\n",
      "          required: false\n",
      "          schema:\n",
      "            type: string\n",
      "            default: None\n",
      "          description: This is the api key that ChatGPT will use if it is called\n",
      "      responses:\n",
      "        '200':\n",
      "          description: A list of items\n",
      "          content:\n",
      "            application/json:\n",
      "              schema:\n",
      "                type: array\n",
      "                items:\n",
      "                  type: string\n"
     ]
    }
   ],
   "source": [
    "with open('swagger2.yaml', 'r') as file:\n",
    "    # Read the entire file content\n",
    "    file_content = file.read()\n",
    "\n",
    "# Print the content or use it in your application\n",
    "print(file_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "de17392a-10b2-4567-84bd-93a57d4427d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = file_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "c8d321d5-7847-401e-96d6-61e78d6adfc5",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'pretrained_models/roberta_st2'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[142], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m      2\u001b[0m directory_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpretrained_models/roberta_st2\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m filename \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirectory_path\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;66;03m# Construct full file path\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     file_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(directory_path, filename)\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;66;03m# Check if it's a file (and not a directory)\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'pretrained_models/roberta_st2'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "directory_path = 'pretrained_models/roberta_st2'\n",
    "for filename in os.listdir(directory_path):\n",
    "    # Construct full file path\n",
    "    file_path = os.path.join(directory_path, filename)\n",
    "    # Check if it's a file (and not a directory)\n",
    "    if os.path.isdir(file_path):\n",
    "        print(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "77e0431e-5aa3-48e2-b697-fbf4a9a6ac2b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['roberta_st1_best_model']\n",
      "  enum:\n",
      "              - roberta_st1_best_model\n",
      "          description:\n",
      " st1\n",
      "          required: false\n",
      "          schema:\n",
      "            type: string\n",
      "            default: roberta_st1_best_model\n",
      "            enum:\n",
      "              - roberta_st1_best_model\n",
      "          description: Determines which model should be used for subtask 1 if subtask 1 is being done\n",
      "        - in: query\n",
      "          \n",
      "['roberta_st2_epoch_9']\n",
      "  enum:\n",
      "              - roberta_st2_epoch_9\n",
      "          description:\n",
      " st2\n",
      "          required: false\n",
      "          schema:\n",
      "            type: string\n",
      "            default: roberta_st2_epoch_9\n",
      "            enum:\n",
      "              - roberta_st2_epoch_9\n",
      "          description: Determines which model should be used for subtask 2 if subtask 2 is being done\n",
      "        - in: query\n",
      "          \n",
      "['rebel_st3_model_our_data_gpt_augmented.pth']\n",
      "enum:\n",
      "              - rebel_st3_model_our_data_gpt_augmented.pth\n",
      "          description:\n",
      "[' st3\\n          required: false\\n          schema:\\n            type: string\\n            default: rebel_st3_model_our_data_gpt_augmented.pth\\n            ', '\\n              - rebel\\n              - gpt\\n              - none\\n          ', ' Determines which model should be used for subtask 3 if subtask 3 is being done\\n        - in: query\\n          ']\n",
      " st3\n",
      "          required: false\n",
      "          schema:\n",
      "            type: string\n",
      "            default: rebel_st3_model_our_data_gpt_augmented.pth\n",
      "            enum:\n",
      "              - rebel_st3_model_our_data_gpt_augmented.pth\n",
      "          description:\n",
      "              - rebel\n",
      "              - gpt\n",
      "              - none\n",
      "          \n",
      "openapi: 3.0.3\n",
      "info:\n",
      "  title: Relation Detection - OpenAPI 3.3\n",
      "  description: Here we provide a platform to run and compare multiple models against each other\n",
      "  version: 1.0.11\n",
      "paths:\n",
      "  \n",
      "  /Flag_List:\n",
      "    get:\n",
      "      summary: Get items with individual choices\n",
      "      parameters:\n",
      "        - in: query\n",
      "          name: ut\n",
      "          required: false\n",
      "          schema:\n",
      "            type: string\n",
      "          description: Type here if you want to input your own sentences; please seperate each sentence with a period followed by a space.\n",
      "        - in: query\n",
      "          name: st1\n",
      "          required: false\n",
      "          schema:\n",
      "            type: string\n",
      "            default: roberta_st1_best_model\n",
      "            enum:\n",
      "              - roberta_st1_best_model\n",
      "          description: Determines which model should be used for subtask 1 if subtask 1 is being done\n",
      "        - in: query\n",
      "          name: st2\n",
      "          required: false\n",
      "          schema:\n",
      "            type: string\n",
      "            default: roberta_st2_epoch_9\n",
      "            enum:\n",
      "              - roberta_st2_epoch_9\n",
      "          description: Determines which model should be used for subtask 2 if subtask 2 is being done\n",
      "        - in: query\n",
      "          name: st3\n",
      "          required: false\n",
      "          schema:\n",
      "            type: string\n",
      "            default: rebel_st3_model_our_data_gpt_augmented.pth\n",
      "            enum:\n",
      "              - rebel_st3_model_our_data_gpt_augmented.pth\n",
      "          description: Determines which model should be used for subtask 3 if subtask 3 is being done\n",
      "        - in: query\n",
      "          name: api\n",
      "          required: false\n",
      "          schema:\n",
      "            type: string\n",
      "            default: None\n",
      "          description: This is the api key that ChatGPT will use if it is called\n",
      "      responses:\n",
      "        '200':\n",
      "          description: A list of items\n",
      "          content:\n",
      "            application/json:\n",
      "              schema:\n",
      "                type: array\n",
      "                items:\n",
      "                  type: string\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "x = s.split('name:')\n",
    "final_string = x[0]\n",
    "final_string = final_string + 'name:' + x[1]\n",
    "#print(x)\n",
    "flag = False\n",
    "for sent in x:\n",
    "    if flag == True:\n",
    "        final_string = final_string + 'name:' + sent\n",
    "    if sent[:4] == ' st1':\n",
    "        options = []\n",
    "        directory_path = 'pretrained_models/st1'\n",
    "        for filename in os.listdir(directory_path):\n",
    "            file_path = os.path.join(directory_path, filename)\n",
    "            if os.path.isdir(file_path):\n",
    "                if filename != '.DS_Store':\n",
    "                    prefix = filename + '_'\n",
    "                    for modelname in os.listdir(file_path):\n",
    "                        if os.path.isdir(file_path):\n",
    "                            \n",
    "                            if modelname != '.DS_Store':\n",
    "                                option = prefix + modelname\n",
    "                                options.append(option)\n",
    "        print(options)\n",
    "        parts = re.split(r'enum:|description:', sent)\n",
    "        st1_str = '  enum:\\n'\n",
    "        for i in options:\n",
    "            st1_str = st1_str + '              - ' + i + '\\n'\n",
    "        st1_str = st1_str + '          description:'\n",
    "        print(st1_str)\n",
    "        print(parts[0] + st1_str + parts[1])\n",
    "        final_st1 = 'name:' + parts[0] + st1_str + parts[1]\n",
    "        final_string = final_string + final_st1\n",
    "    \n",
    "    if sent[:4] == ' st2':\n",
    "        options = []\n",
    "        directory_path = 'pretrained_models/st2'\n",
    "        for filename in os.listdir(directory_path):\n",
    "            file_path = os.path.join(directory_path, filename)\n",
    "            if os.path.isdir(file_path):\n",
    "                if filename != '.DS_Store':\n",
    "                    prefix = filename + '_'\n",
    "                    for modelname in os.listdir(file_path):\n",
    "                        if os.path.isdir(file_path):\n",
    "                            \n",
    "                            if modelname != '.DS_Store':\n",
    "                                option = prefix + modelname\n",
    "                                options.append(option)\n",
    "        print(options)\n",
    "        parts = re.split(r'enum:|description:', sent)\n",
    "        st2_str = '  enum:\\n'\n",
    "        for i in options:\n",
    "            st2_str = st2_str + '              - ' + i + '\\n'\n",
    "        st2_str = st2_str + '          description:'\n",
    "        print(st2_str)\n",
    "        print(parts[0] + st2_str + parts[1])\n",
    "        final_st2 = 'name:' + parts[0] + st2_str + parts[1]\n",
    "        final_string = final_string + final_st2\n",
    "    if sent[:4] == ' st3':\n",
    "        options = []\n",
    "        directory_path = 'pretrained_models/st3'\n",
    "        for filename in os.listdir(directory_path):\n",
    "            file_path = os.path.join(directory_path, filename)\n",
    "            if os.path.isdir(file_path):\n",
    "                if filename != '.DS_Store':\n",
    "                    prefix = filename + '_'\n",
    "                    for modelname in os.listdir(file_path):\n",
    "                        if os.path.isdir(file_path):\n",
    "                            \n",
    "                            if modelname != '.DS_Store':\n",
    "                                option = prefix + modelname\n",
    "                                options.append(option)\n",
    "        print(options)\n",
    "        parts = re.split(r'enum:|description:', sent)\n",
    "        st3_str = 'enum:\\n'\n",
    "        for i in options:\n",
    "            st3_str = st3_str + '              - ' + i + '\\n'\n",
    "        st3_str = st3_str + '          description:'\n",
    "        print(st3_str)\n",
    "        print(parts)\n",
    "        print(parts[0] + st3_str + parts[1])\n",
    "        final_st3 = 'name:' + parts[0] + st3_str + parts[2]\n",
    "        final_string = final_string + final_st3\n",
    "        flag = True\n",
    "    \n",
    "\n",
    "\n",
    "print(final_string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "b1776c5f-dabe-48cf-ae69-9bb2e9db1dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "data = yaml.safe_load(final_string)\n",
    "with open('output.yaml', 'w') as file:\n",
    "    yaml.dump(data, file, default_flow_style=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "851ae34c-801f-442f-a08b-fc39036a27c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "openapi: 3.0.3\n",
      "info:\n",
      "  title: Relation Detection - OpenAPI 3.3\n",
      "  description: Here we provide a platform to run and compare multiple models against each other\n",
      "  version: 1.0.11\n",
      "paths:\n",
      "  \n",
      "  /Flag_List:\n",
      "    get:\n",
      "      summary: Get items with individual choices\n",
      "      parameters:\n",
      "        - in: query\n",
      "          name: st1\n",
      "          required: false\n",
      "          schema:\n",
      "            type: string\n",
      "            default: roberta_st1_best_model\n",
      "            enum:\n",
      "              - roberta_st1_best_model\n",
      "          description: Determines which model should be used for subtask 1 if subtask 1 is being done\n",
      "        - in: query\n",
      "          name: st2\n",
      "          required: false\n",
      "          schema:\n",
      "            type: string\n",
      "            default: roberta_st2_epoch_9\n",
      "            enum:\n",
      "              - roberta_st2_epoch_9\n",
      "          description: Determines which model should be used for subtask 2 if subtask 2 is being done\n",
      "        - in: query\n",
      "          name: st3\n",
      "          required: false\n",
      "          schema:\n",
      "            type: string\n",
      "            default: rebel_st3_model_our_data_gpt_augmented.pth\n",
      "            enum:\n",
      "              - rebel_st3_model_our_data_gpt_augmented.pth\n",
      "          description: Determines which model should be used for subtask 3 if subtask 3 is being done\n",
      "        - in: query\n",
      "          name: api\n",
      "          required: false\n",
      "          schema:\n",
      "            type: string\n",
      "            default: None\n",
      "          description: This is the api key that ChatGPT will use if it is called\n",
      "      responses:\n",
      "        '200':\n",
      "          description: A list of items\n",
      "          content:\n",
      "            application/json:\n",
      "              schema:\n",
      "                type: array\n",
      "                items:\n",
      "                  type: string\n",
      "changes have been made\n"
     ]
    }
   ],
   "source": [
    "def set_choices(yaml_file):\n",
    "    with open(yaml_file, 'r') as file:\n",
    "    # Read the entire file content\n",
    "        file_content = file.read()\n",
    "    s = file_content\n",
    "    x = s.split('name:')\n",
    "    final_string = x[0]\n",
    "    #print(x)\n",
    "    flag = False\n",
    "    for sent in x:\n",
    "        if flag == True:\n",
    "            final_string = final_string + 'name:' + sent\n",
    "        if sent[:4] == ' st1':\n",
    "            options = []\n",
    "            directory_path = 'pretrained_models/st1'\n",
    "            for filename in os.listdir(directory_path):\n",
    "                file_path = os.path.join(directory_path, filename)\n",
    "                if os.path.isdir(file_path):\n",
    "                    if filename != '.DS_Store':\n",
    "                        prefix = filename + '_'\n",
    "                        for modelname in os.listdir(file_path):\n",
    "                            if os.path.isdir(file_path):\n",
    "                                if modelname != '.DS_Store':\n",
    "                                    option = prefix + modelname\n",
    "                                    options.append(option)\n",
    "            parts = re.split(r'enum:|description:', sent)\n",
    "            st1_str = '  enum:\\n'\n",
    "            for i in options:\n",
    "                st1_str = st1_str + '              - ' + i + '\\n'\n",
    "            st1_str = st1_str + '          description:'\n",
    "            final_st1 = 'name:' + parts[0] + st1_str + parts[1]\n",
    "            final_string = final_string + final_st1\n",
    "        \n",
    "        if sent[:4] == ' st2':\n",
    "            options = []\n",
    "            directory_path = 'pretrained_models/st2'\n",
    "            for filename in os.listdir(directory_path):\n",
    "                file_path = os.path.join(directory_path, filename)\n",
    "                if os.path.isdir(file_path):\n",
    "                    if filename != '.DS_Store':\n",
    "                        prefix = filename + '_'\n",
    "                        for modelname in os.listdir(file_path):\n",
    "                            if os.path.isdir(file_path):\n",
    "                                \n",
    "                                if modelname != '.DS_Store':\n",
    "                                    option = prefix + modelname\n",
    "                                    options.append(option)\n",
    "            parts = re.split(r'enum:|description:', sent)\n",
    "            st2_str = '  enum:\\n'\n",
    "            for i in options:\n",
    "                st2_str = st2_str + '              - ' + i + '\\n'\n",
    "            st2_str = st2_str + '          description:'\n",
    "            final_st2 = 'name:' + parts[0] + st2_str + parts[1]\n",
    "            final_string = final_string + final_st2\n",
    "        if sent[:4] == ' st3':\n",
    "            options = []\n",
    "            directory_path = 'pretrained_models/st3'\n",
    "            for filename in os.listdir(directory_path):\n",
    "                file_path = os.path.join(directory_path, filename)\n",
    "                if os.path.isdir(file_path):\n",
    "                    if filename != '.DS_Store':\n",
    "                        prefix = filename + '_'\n",
    "                        for modelname in os.listdir(file_path):\n",
    "                            if os.path.isdir(file_path):\n",
    "                                \n",
    "                                if modelname != '.DS_Store':\n",
    "                                    option = prefix + modelname\n",
    "                                    options.append(option)\n",
    "            parts = re.split(r'enum:|description:', sent)\n",
    "            st3_str = 'enum:\\n'\n",
    "            for i in options:\n",
    "                st3_str = st3_str + '              - ' + i + '\\n'\n",
    "            st3_str = st3_str + '          description:'\n",
    "            final_st3 = 'name:' + parts[0] + st3_str + parts[2]\n",
    "            final_string = final_string + final_st3\n",
    "            flag = True\n",
    "        \n",
    "    \n",
    "    \n",
    "    print(final_string)\n",
    "    if file_content == final_string:\n",
    "        return\n",
    "    else:\n",
    "        print('changes have been made')\n",
    "        data = yaml.safe_load(final_string)\n",
    "        with open('output.yaml', 'w') as file:\n",
    "            yaml.dump(data, file, default_flow_style=False)\n",
    "        return\n",
    "\n",
    "set_choices('swagger2.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "648eb7fb-ad6d-469d-811c-d8abd77238fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "for i in range(3,4):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f015f1a7-5c4b-4f37-9279-ca2bec472842",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "openapi: 3.0.3\n",
      "info:\n",
      "  title: Relation Detection - OpenAPI 3.3\n",
      "  description: Here we provide a platform to run and compare multiple models against each other\n",
      "  version: 1.0.11\n",
      "paths:\n",
      "  /RunPipeline:\n",
      "    get:\n",
      "      summary: Get items with individual choices\n",
      "      parameters:\n",
      "        - in: query\n",
      "          name: ut\n",
      "          required: true\n",
      "          schema:\n",
      "            type: string\n",
      "            default: None\n",
      "          description: Type here if you want to input your own sentences; please separate each sentence with a period followed by a space. If you fill in sentences, then the pipeline will only use those sentences for inference.\n",
      "        - in: query\n",
      "          name: st0\n",
      "          required: true\n",
      "          schema:\n",
      "            type: string\n",
      "            default: roberta_st0_best_model_st1.pt\n",
      "            enum:\n",
      "              - roberta_st0_best_model_st1.pt\n",
      "              - test_st0_test file\n",
      "          description: Determines which pretrained model should be used for subtask 0. Subtask 0 is mandatory\n",
      "        - in: query\n",
      "          name: st1\n",
      "          required: true\n",
      "          schema:\n",
      "            type: string\n",
      "            default: roberta_st1_best_model\n",
      "            enum:\n",
      "              - roberta_st1_best_model\n",
      "              - test_st1_test file\n",
      "              - rebel_st3_model_our_data_gpt_augmented.pth\n",
      "              - zephyr\n",
      "              - dpo\n",
      "              - una\n",
      "              - solar\n",
      "              - gpt4\n",
      "              - off\n",
      "          description: Determines which model should be used for subtask 1 if subtask 1 is being done\n",
      "        - in: query\n",
      "          name: st2\n",
      "          required: true\n",
      "          schema:\n",
      "            type: string\n",
      "            default: roberta_st2_epoch_9\n",
      "            enum:\n",
      "              - roberta_st2_epoch_9\n",
      "              - test_st2_test file\n",
      "              - rebel_st3_model_our_data_gpt_augmented.pth\n",
      "              - zephyr\n",
      "              - dpo\n",
      "              - una\n",
      "              - solar\n",
      "              - gpt4\n",
      "              - off\n",
      "          description: Determines which model should be used for subtask 2 if subtask 2 is being done\n",
      "        - in: query\n",
      "          \n",
      "openapi: 3.0.3\n",
      "info:\n",
      "  title: Relation Detection - OpenAPI 3.3\n",
      "  description: Here we provide a platform to run and compare multiple models against each other\n",
      "  version: 1.0.11\n",
      "paths:\n",
      "  /RunPipeline:\n",
      "    get:\n",
      "      summary: Get items with individual choices\n",
      "      parameters:\n",
      "        - in: query\n",
      "          name: ut\n",
      "          required: true\n",
      "          schema:\n",
      "            type: string\n",
      "            default: None\n",
      "          description: Type here if you want to input your own sentences; please separate each sentence with a period followed by a space. If you fill in sentences, then the pipeline will only use those sentences for inference.\n",
      "        - in: query\n",
      "          name: st0\n",
      "          required: true\n",
      "          schema:\n",
      "            type: string\n",
      "            default: roberta_st0_best_model_st1.pt\n",
      "            enum:\n",
      "              - roberta_st0_best_model_st1.pt\n",
      "              - test_st0_test file\n",
      "          description: Determines which pretrained model should be used for subtask 0. Subtask 0 is mandatory\n",
      "        - in: query\n",
      "          name: st1\n",
      "          required: true\n",
      "          schema:\n",
      "            type: string\n",
      "            default: roberta_st1_best_model\n",
      "            enum:\n",
      "              - roberta_st1_best_model\n",
      "              - test_st1_test file\n",
      "              - rebel_st3_model_our_data_gpt_augmented.pth\n",
      "              - zephyr\n",
      "              - dpo\n",
      "              - una\n",
      "              - solar\n",
      "              - gpt4\n",
      "              - off\n",
      "          description: Determines which model should be used for subtask 1 if subtask 1 is being done\n",
      "        - in: query\n",
      "          name: st2\n",
      "          required: true\n",
      "          schema:\n",
      "            type: string\n",
      "            default: roberta_st2_epoch_9\n",
      "            enum:\n",
      "              - roberta_st2_epoch_9\n",
      "              - test_st2_test file\n",
      "              - rebel_st3_model_our_data_gpt_augmented.pth\n",
      "              - zephyr\n",
      "              - dpo\n",
      "              - una\n",
      "              - solar\n",
      "              - gpt4\n",
      "              - off\n",
      "          description: Determines which model should be used for subtask 2 if subtask 2 is being done\n",
      "        - in: query\n",
      "          name: t_f\n",
      "          required: true\n",
      "          schema:\n",
      "            type: string\n",
      "            default: test.csv\n",
      "            enum:\n",
      "              - test.csv\n",
      "          description: This is the file that the pipeline will perform an inference on. If you would like to add in new datasets to the options put the csv file in the new_data folder and make sure that the name is unique.\n",
      "        - in: query\n",
      "          name: api\n",
      "          required: true\n",
      "          schema:\n",
      "            type: string\n",
      "            default: None\n",
      "          description: This is the api key that ChatGPT will use if it is called in st3.\n",
      "      responses:\n",
      "        '200':\n",
      "          description: A list of items\n",
      "          content:\n",
      "            application/json:\n",
      "              schema:\n",
      "                type: array\n",
      "                items:\n",
      "                  type: string\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "def set_choices(yaml_file):\n",
    "    with open(yaml_file, 'r') as file:\n",
    "    # Read the entire file content\n",
    "        file_content = file.read()\n",
    "    s = file_content\n",
    "    #print(s)\n",
    "    x = s.split('name:')\n",
    "    #print(x)\n",
    "    final_string = x[0]\n",
    "    final_string = final_string + 'name:' + x[1]\n",
    "    #print(x)\n",
    "    flag = False\n",
    "    for sent in x:\n",
    "        \n",
    "        if flag == True:\n",
    "            final_string = final_string + 'name:' + sent\n",
    "            \n",
    "            \n",
    "            \n",
    "        if sent[:4] == ' st0':\n",
    "            options = []\n",
    "            directory_path = 'pretrained_models/st0'\n",
    "            for filename in os.listdir(directory_path):\n",
    "                file_path = os.path.join(directory_path, filename)\n",
    "                if os.path.isdir(file_path):\n",
    "                    if filename != '.DS_Store':\n",
    "                        prefix = filename + '_'\n",
    "                        for modelname in os.listdir(file_path):\n",
    "                            if os.path.isdir(file_path):\n",
    "                                if modelname != '.DS_Store':\n",
    "                                    option = prefix + modelname\n",
    "                                    options.append(option)\n",
    "            \n",
    "            parts = re.split(r'enum:|description:', sent)\n",
    "            st0_str = 'enum:\\n'\n",
    "            #print(parts)\n",
    "            for i in options:\n",
    "                st0_str = st0_str + '              - ' + i + '\\n'\n",
    "            #st0_str = st0_str + '              - off' + '\\n'\n",
    "            st0_str = st0_str + '          description:'\n",
    "            final_st0 = 'name:' + parts[0] + st0_str + parts[2]\n",
    "            final_string = final_string + final_st0\n",
    "            \n",
    "                        \n",
    "            \n",
    "        if sent[:4] == ' st1':\n",
    "            \n",
    "            st1_str = 'enum:\\n'\n",
    "            directory_list = ['pretrained_models/st1','pretrained_models/st3']\n",
    "            directory_path = 'pretrained_models/st1'\n",
    "            for directory_path in directory_list:\n",
    "                options = []\n",
    "                for filename in os.listdir(directory_path):\n",
    "                    file_path = os.path.join(directory_path, filename)\n",
    "                    if os.path.isdir(file_path):\n",
    "                        if filename != '.DS_Store':\n",
    "                            prefix = filename + '_'\n",
    "                            for modelname in os.listdir(file_path):\n",
    "                                if os.path.isdir(file_path):\n",
    "                                    if modelname != '.DS_Store':\n",
    "                                        option = prefix + modelname\n",
    "                                        options.append(option)\n",
    "                \n",
    "                parts = re.split(r'enum:|description:', sent)\n",
    "                \n",
    "                #print(parts)\n",
    "                for i in options:\n",
    "                    st1_str = st1_str + '              - ' + i + '\\n'\n",
    "            \n",
    "            st1_str = st1_str + '              - zephyr' + '\\n'\n",
    "            st1_str = st1_str + '              - dpo' + '\\n'\n",
    "            st1_str = st1_str + '              - una' + '\\n'\n",
    "            st1_str = st1_str + '              - solar' + '\\n'\n",
    "            st1_str = st1_str + '              - gpt4' + '\\n'\n",
    "            st1_str = st1_str + '              - off' + '\\n'\n",
    "            st1_str = st1_str + '          description:'\n",
    "            final_st1 = 'name:' + parts[0] + st1_str + parts[2]\n",
    "            final_string = final_string + final_st1\n",
    "        if sent[:4] == ' st2':\n",
    "            options = []\n",
    "            directory_list = ['pretrained_models/st2','pretrained_models/st3']\n",
    "            #directory_path = 'pretrained_models/st2'\n",
    "            st2_str = 'enum:\\n'\n",
    "            for directory_path in directory_list:\n",
    "                options = []\n",
    "                #print('here')\n",
    "                #print(directory_path)\n",
    "                #print('here')\n",
    "                for filename in os.listdir(directory_path):\n",
    "                    file_path = os.path.join(directory_path, filename)\n",
    "                    if os.path.isdir(file_path):\n",
    "                        if filename != '.DS_Store':\n",
    "                            prefix = filename + '_'\n",
    "                            for modelname in os.listdir(file_path):\n",
    "                                if os.path.isdir(file_path):\n",
    "                                    \n",
    "                                    if modelname != '.DS_Store':\n",
    "                                        option = prefix + modelname\n",
    "                                        options.append(option)\n",
    "                parts = re.split(r'enum:|description:', sent)\n",
    "                #print(parts)\n",
    "                \n",
    "                #print(options)\n",
    "                for i in options:\n",
    "                    st2_str = st2_str + '              - ' + i + '\\n'\n",
    "                #print(st2_str)\n",
    "            \n",
    "            st2_str = st2_str + '              - zephyr' + '\\n'\n",
    "            st2_str = st2_str + '              - dpo' + '\\n'\n",
    "            st2_str = st2_str + '              - una' + '\\n'\n",
    "            st2_str = st2_str + '              - solar' + '\\n'\n",
    "            st2_str = st2_str + '              - gpt4' + '\\n'\n",
    "            st2_str = st2_str + '              - off' + '\\n'\n",
    "            st2_str = st2_str + '          description:'\n",
    "            final_st2 = 'name:' + parts[0] + st2_str + parts[2]\n",
    "            final_string = final_string + final_st2\n",
    "            print(final_string)\n",
    "        '''\n",
    "        if sent[:4] == ' st3':\n",
    "            options = []\n",
    "            directory_path = 'pretrained_models/st3'\n",
    "            for filename in os.listdir(directory_path):\n",
    "                file_path = os.path.join(directory_path, filename)\n",
    "                if os.path.isdir(file_path):\n",
    "                    if filename != '.DS_Store':\n",
    "                        prefix = filename + '_'\n",
    "                        for modelname in os.listdir(file_path):\n",
    "                            if os.path.isdir(file_path):\n",
    "                                \n",
    "                                if modelname != '.DS_Store':\n",
    "                                    option = prefix + modelname\n",
    "                                    options.append(option)\n",
    "            parts = re.split(r'enum:|description:', sent)\n",
    "            st3_str = 'enum:\\n'\n",
    "            for i in options:\n",
    "                st3_str = st3_str + '              - ' + i + '\\n'\n",
    "            st3_str = st3_str + '              - off' + '\\n'\n",
    "            st3_str = st3_str + '              - zephyr' + '\\n'\n",
    "            st3_str = st3_str + '              - dpo' + '\\n'\n",
    "            st3_str = st3_str + '              - una' + '\\n'\n",
    "            st3_str = st3_str + '              - solar' + '\\n'\n",
    "            st3_str = st3_str + '              - gpt4' + '\\n'\n",
    "            st3_str = st3_str + '          description:'\n",
    "            final_st3 = 'name:' + parts[0] + st3_str + parts[2]\n",
    "            final_string = final_string + final_st3\n",
    "            #flag = True\n",
    "            \n",
    "         '''   \n",
    "        if sent[:4] == ' t_f':\n",
    "            options = []\n",
    "            directory_path = 'new_data/'\n",
    "            for filename in os.listdir(directory_path):\n",
    "                file_path = os.path.join(directory_path, filename)\n",
    "                if os.path.isfile(file_path):\n",
    "                    if filename != '.DS_Store':\n",
    "                        option = filename\n",
    "                        #print(option)\n",
    "                        options.append(option)\n",
    "            parts = re.split(r'enum:|description:', sent)\n",
    "            #print(parts)\n",
    "            tf_str = 'enum:\\n'\n",
    "            for i in options:\n",
    "                tf_str = tf_str + '              - ' + i + '\\n'\n",
    "            tf_str = tf_str + '          description:'\n",
    "            final_tf = 'name:' + parts[0] + tf_str + parts[2]\n",
    "            final_string = final_string + final_tf\n",
    "            flag = True\n",
    "    print(final_string)\n",
    "\n",
    "    #print(final_string)\n",
    "    if file_content == final_string:\n",
    "        return\n",
    "    else:\n",
    "        print('changes have been made')\n",
    "        print(final_string)\n",
    "        fs = final_string\n",
    "        print(fs)\n",
    "        #data = yaml.load(fs,Loader=yaml.BaseLoader)\n",
    "        #with open('static/swagger.yaml', 'w') as f:\n",
    "        with open('static/swagger.yaml', 'w') as file:\n",
    "            file.write(fs)\n",
    "        #with open('output5.yaml', 'w') as f:\n",
    "            #yaml.dump(data, f)\n",
    "        return\n",
    "\n",
    "set_choices('static/swagger.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea75b08-1c3b-4478-bd00-c5fcd1f5c2dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
